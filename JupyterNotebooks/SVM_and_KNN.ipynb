{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8rD0d_GUX_M"
      },
      "source": [
        "# Setup with Conda\n",
        "First install the libmamba solver for Conda\n",
        "```sh\n",
        "    conda update -n base conda\n",
        "    conda install -n base conda-libmamba-solver\n",
        "    conda config --set solver libmamba\n",
        "```\n",
        "\n",
        "Next create a new Conda Environment with the instances\n",
        "```sh\n",
        "    conda create --solver=libmamba -n rapids-23.10 -c rapidsai -c conda-forge -c nvidia  \\\n",
        "    rapids=23.10 python=3.10 cuda-version=12.0\n",
        "```\n",
        "\n",
        "Finally install pip\n",
        "```sh\n",
        "    conda install pip\n",
        "```\n",
        "\n",
        "Use pip to install any other missing packages/modules for this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwnq02UQKe-P"
      },
      "outputs": [],
      "source": [
        "!pip install facenet_pytorch\n",
        "!pip install keras_facenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LjTpK09z2zY"
      },
      "outputs": [],
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bhnl9U8UX_O"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from os import listdir\n",
        "from numpy import load\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from sklearn.preprocessing import StandardScaler as StandardScaler_C\n",
        "from sklearn.preprocessing import MinMaxScaler as MinMaxScaler_C\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier_C\n",
        "from sklearn.preprocessing import LabelEncoder as LabelEncoder_C\n",
        "from sklearn import metrics as metrics_C\n",
        "# from keras.models import load_model\n",
        "from keras_facenet import FaceNet\n",
        "# from mtcnn.mtcnn import MTCNN\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from cuml import KMeans\n",
        "from cuml.cluster import KMeans\n",
        "from cuml.metrics.accuracy import accuracy_score\n",
        "#from cuml.dask.preprocessing.LabelEncoder import LabelEncoder\n",
        "from cuml.preprocessing import LabelEncoder\n",
        "from cuml.svm import LinearSVC\n",
        "from cuml.preprocessing import Normalizer\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "USE_CUDA = True\n",
        "\n",
        "if (USE_CUDA):\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp8KLhKQUX_R"
      },
      "outputs": [],
      "source": [
        "# extracts faces from entire LFW dataset using MTCNN\n",
        "''' preprocessing_file = '../mtcnn_extracted_faces/'\n",
        "from_file = '../LFW_Dataset/lfw-deepfunneled/lfw-deepfunneled/' '''\n",
        "mtcnn = MTCNN(post_process=False, device=device)\n",
        "preprocessing_file = './mtcnn_extracted_faces/'\n",
        "from_file = './lfw-deepfunneled/'\n",
        "\n",
        "\n",
        "list_directories = os.listdir(from_file)\n",
        "for dir in list_directories:\n",
        "  save_path = preprocessing_file + dir + '/'\n",
        "  if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "  curr_path = from_file + dir\n",
        "  curr_files = [curr_path + '/' + image for image in os.listdir(curr_path)]\n",
        "  for i, curr_img in enumerate(curr_files):\n",
        "    frame = Image.open(curr_img).convert(\"RGB\")\n",
        "    face = mtcnn(frame)\n",
        "\n",
        "    if face is None:\n",
        "      continue\n",
        "    img=Image.fromarray(np.uint8(face.permute(1,2,0).int().numpy()))\n",
        "    save_name = save_path + dir + str(i).zfill(4) + '.jpg'\n",
        "    img.save(save_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIxrm16xbRTe"
      },
      "outputs": [],
      "source": [
        "#!rm -rf lfw-deepfunneled/\n",
        "#!tar -xvzf lfw-deepfunneled.tgz\n",
        "#!tar -cvzf mtcnn_extracted_faces.tar.gz mtcnn_extracted_faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Yh2nRdUX_S"
      },
      "outputs": [],
      "source": [
        "def load_dataset(directory, batch_size=1000):\n",
        "  train_x, train_y, test_x, test_y = [],[], [],[]\n",
        "  batch_num = 0\n",
        "\n",
        "  # add slash to end of dir path\n",
        "  if directory[-1] != '/':\n",
        "    directory += '/'\n",
        "\n",
        "  for i, subdir in enumerate(listdir(directory)):\n",
        "\n",
        "    if ((i % batch_size) == 0) and i != 0:\n",
        "      savez_compressed('lfw-deepfunneled-dataset_{}.npz'.format(str(batch_num).zfill(4)),train_x,train_y,test_x,test_y)\n",
        "\n",
        "      train_x, train_y, test_x, test_y = [],[], [],[]\n",
        "      batch_num += 1\n",
        "\n",
        "    path = directory + subdir + '/'\n",
        "    #load all faces in subdirectory\n",
        "    faces = [asarray(Image.open(path + img_name).convert(\"RGB\")) for img_name in listdir(path)]\n",
        "    if len(faces)>1:\n",
        "      test_x.append(faces.pop())\n",
        "      test_y.append(subdir)\n",
        "    labels = [subdir for _ in range(len(faces))]\n",
        "    # print(\"%d There are %d images in the class %s:\"%(i,len(faces),subdir))\n",
        "    train_x.extend(faces)\n",
        "    train_y.extend(labels)\n",
        "  # return asarray(train_x),asarray(train_y), asarray(test_x), asarray(test_y)\n",
        "\n",
        "  if not (train_x == [] and train_y == [] and test_x == [] and test_y == []):\n",
        "    savez_compressed('lfw-deepfunneled-dataset_{}.npz'.format(str(batch_num).zfill(4)),train_x,train_y,test_x,test_y)\n",
        "\n",
        "\n",
        "load_dataset('./mtcnn_extracted_faces/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amq1V-tNUX_S",
        "outputId": "ca7a2051-e81f-4572-d40f-43f93a776478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lfw-deepfunneled-dataset_0002.npz', 'lfw-deepfunneled-dataset_0005.npz', 'lfw-deepfunneled-dataset_0001.npz', 'lfw-deepfunneled-dataset_0004.npz', 'lfw-deepfunneled-dataset_0003.npz', 'lfw-deepfunneled-dataset_0000.npz']\n",
            "(1798, 160, 160, 3) (1798,) (304, 160, 160, 3) (304,)\n",
            "57/57 [==============================] - 4s 38ms/step\n",
            "10/10 [==============================] - 0s 35ms/step\n",
            "new_trainx (1798, 512) | new_testx (304, 512) | dtype float32\n",
            "new_trainy 1798 | new_testy 304 \n",
            "(1219, 160, 160, 3) (1219,) (204, 160, 160, 3) (204,)\n",
            "39/39 [==============================] - 2s 39ms/step\n",
            "7/7 [==============================] - 0s 35ms/step\n",
            "new_trainx (3017, 512) | new_testx (508, 512) | dtype float32\n",
            "new_trainy 3017 | new_testy 508 \n",
            "(1903, 160, 160, 3) (1903,) (302, 160, 160, 3) (302,)\n",
            "60/60 [==============================] - 2s 39ms/step\n",
            "10/10 [==============================] - 0s 36ms/step\n",
            "new_trainx (4920, 512) | new_testx (810, 512) | dtype float32\n",
            "new_trainy 4920 | new_testy 810 \n",
            "(2028, 160, 160, 3) (2028,) (296, 160, 160, 3) (296,)\n",
            "64/64 [==============================] - 2s 38ms/step\n",
            "10/10 [==============================] - 0s 36ms/step\n",
            "new_trainx (6948, 512) | new_testx (1106, 512) | dtype float32\n",
            "new_trainy 6948 | new_testy 1106 \n",
            "(2053, 160, 160, 3) (2053,) (275, 160, 160, 3) (275,)\n",
            "65/65 [==============================] - 3s 39ms/step\n",
            "9/9 [==============================] - 0s 36ms/step\n",
            "new_trainx (9001, 512) | new_testx (1381, 512) | dtype float32\n",
            "new_trainy 9001 | new_testy 1381 \n",
            "(2552, 160, 160, 3) (2552,) (298, 160, 160, 3) (298,)\n",
            "80/80 [==============================] - 3s 38ms/step\n",
            "10/10 [==============================] - 0s 36ms/step\n",
            "new_trainx (11553, 512) | new_testx (1679, 512) | dtype float32\n",
            "new_trainy 11553 | new_testy 1679 \n",
            "Final new_trainx size (11553, 512) | Final new_testx size (1679, 512)\n",
            "Final new_trainy size (11553,) | Final new_testy size (1679,)\n"
          ]
        }
      ],
      "source": [
        "#create and save embeddings\n",
        "embedder = FaceNet()\n",
        "#load the compressed dataset and facenet keras model\n",
        "dataset_npz_filenames = []\n",
        "reg_expr = '^lfw-deepfunneled-dataset_[0-9]{4}.npz'\n",
        "for file in listdir('./'):\n",
        "    if re.search(reg_expr, file):\n",
        "        dataset_npz_filenames.append(file)\n",
        "\n",
        "print(dataset_npz_filenames)\n",
        "new_trainy, new_testy  = [], []\n",
        "new_trainx = new_testx = np.zeros((0,512), dtype='float32')\n",
        "for i, file in enumerate(dataset_npz_filenames):\n",
        "    data = load(file)\n",
        "    trainx, trainy, testx, testy = data['arr_0'], data['arr_1'], data['arr_2'],  data['arr_3']\n",
        "    print(trainx.shape,trainy.shape,testx.shape, testy.shape)\n",
        "\n",
        "    new_trainx = np.vstack((new_trainx, embedder.embeddings(trainx)))\n",
        "    new_testx = np.vstack((new_testx, embedder.embeddings(testx)))\n",
        "    print('new_trainx {} | new_testx {} | dtype {}'.format(new_trainx.shape, new_testx.shape, new_testx.dtype))\n",
        "\n",
        "    for el in trainy:\n",
        "        new_trainy.append(el)\n",
        "    for el in testy:\n",
        "        new_testy.append(el)\n",
        "    print('new_trainy {} | new_testy {} '.format(len(new_trainy), len(new_testy)))\n",
        "\n",
        "new_trainy=np.array(new_trainy)\n",
        "new_testy=np.array(new_testy)\n",
        "\n",
        "#save the embeddings\n",
        "#compress the 512 embeddings of each face\n",
        "print(\"Final new_trainx size {} | Final new_testx size {}\".format(new_trainx.shape, new_testx.shape))\n",
        "print(\"Final new_trainy size {} | Final new_testy size {}\".format(new_trainy.shape, new_testy.shape))\n",
        "savez_compressed('lfw-deepfunneled-embeddings.npz',new_trainx,new_trainy,new_testx,new_testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0TBqe1tUX_T"
      },
      "outputs": [],
      "source": [
        "# Load the compressed dataset and embeddings\n",
        "data = np.load('./lfw-deepfunneled-embeddings.npz')\n",
        "train_X, train_Y, test_X, test_Y = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "q7A2CN3lU9KZ",
        "outputId": "6829813c-41b0-46c5-a75e-bffee6571686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9962780475616455\n",
            "0.8439547419548035\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' #display\\ntrainy_list = list(trainy)\\nselected_idx=0\\np=int(predict_train[selected_idx])\\n\\n# if p in trainy_list:\\nval = trainy_list.index(p)\\n\\n#display Predicated data\\nplt.subplot(1,2,2)\\nplt.imshow(train_x[val])\\nplt.title(train_y[val])\\nplt.xlabel(\"Predicted Data\")\\n\\n#print(train_y) '"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Linear SVC Results\n",
        "\n",
        "model=LinearSVC(C=10)\n",
        "\n",
        "#MinMaxScaling\n",
        "scaler=MinMaxScaler_C().fit(train_X)\n",
        "trainx =scaler.transform(train_X)\n",
        "testx = scaler.transform(test_X)\n",
        "\n",
        "#encode labels\n",
        "label_encoder = LabelEncoder().fit(train_Y)\n",
        "true_training_labels_encoded = label_encoder.transform(train_Y)\n",
        "\n",
        "model.fit(cp.asarray(trainx),cp.asarray(true_training_labels_encoded))\n",
        "\n",
        "\n",
        "#predict\n",
        "predict_train = model.predict(cp.asarray(trainx))\n",
        "predict_test = model.predict(cp.asarray(testx))\n",
        "\n",
        "#Accuracy\n",
        "true_test_labels_encoded = label_encoder.transform(test_Y)\n",
        "acc_train = accuracy_score(true_training_labels_encoded,predict_train)\n",
        "acc_test = accuracy_score(true_test_labels_encoded,predict_test)\n",
        "\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxNv7NeKC8qA",
        "outputId": "ded63af3-89de-4bbe-adec-798724757129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model at for training is 1.0\n",
            "Accuracy of model at for testing is 0.8826682549136391\n"
          ]
        }
      ],
      "source": [
        "#KNN-CPU\n",
        "\n",
        "#Scaling\n",
        "scaler=MinMaxScaler_C().fit(train_X)\n",
        "trainx =scaler.transform(train_X)\n",
        "testx = scaler.transform(test_X)\n",
        "\n",
        "#encode labels\n",
        "label_encoder = LabelEncoder_C().fit(train_Y)\n",
        "true_training_labels_encoded = label_encoder.transform(train_Y)\n",
        "true_test_labels_encoded = label_encoder.transform(test_Y)\n",
        "\n",
        "#draw graph\n",
        "N=trainx.shape[0]\n",
        "k = 1\n",
        "neigh = KNeighborsClassifier_C(n_neighbors = k).fit(trainx,true_training_labels_encoded)\n",
        "predict_train = neigh.predict(trainx)\n",
        "predict_test = neigh.predict(testx)\n",
        "print(\"Accuracy of model at for training is\",metrics_C.accuracy_score(true_training_labels_encoded, predict_train))\n",
        "print(\"Accuracy of model at for testing is\",metrics_C.accuracy_score(true_test_labels_encoded, predict_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "SVM and KNN.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}