{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8rD0d_GUX_M"
      },
      "source": [
        "# Setup with Conda\n",
        "First install the libmamba solver for Conda\n",
        "```sh\n",
        "    conda update -n base conda\n",
        "    conda install -n base conda-libmamba-solver\n",
        "    conda config --set solver libmamba\n",
        "```\n",
        "\n",
        "Next create a new Conda Environment with the instances\n",
        "```sh\n",
        "    conda create --solver=libmamba -n rapids-23.10 -c rapidsai -c conda-forge -c nvidia  \\\n",
        "    rapids=23.10 python=3.10 cuda-version=12.0\n",
        "```\n",
        "\n",
        "Finally install pip\n",
        "```sh\n",
        "    conda install pip\n",
        "```\n",
        "\n",
        "Use pip to install any other missing packages/modules for this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwnq02UQKe-P"
      },
      "outputs": [],
      "source": [
        "!pip install facenet_pytorch\n",
        "!pip install keras_facenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LjTpK09z2zY"
      },
      "outputs": [],
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Bhnl9U8UX_O"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 21:54:26.145865: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-14 21:54:26.182631: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-14 21:54:26.182663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-14 21:54:26.183392: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-14 21:54:26.188488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-14 21:54:26.941999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from os import listdir\n",
        "from numpy import load\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from sklearn.preprocessing import StandardScaler as StandardScaler_C\n",
        "from sklearn.preprocessing import MinMaxScaler as MinMaxScaler_C\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier_C\n",
        "from sklearn.preprocessing import LabelEncoder as LabelEncoder_C, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics as metrics_C\n",
        "# from keras.models import load_model\n",
        "from keras_facenet import FaceNet\n",
        "# from mtcnn.mtcnn import MTCNN\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import Normalizer\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from cuml import KMeans\n",
        "from cuml.cluster import KMeans\n",
        "from cuml.metrics.accuracy import accuracy_score\n",
        "#from cuml.dask.preprocessing.LabelEncoder import LabelEncoder\n",
        "from cuml.preprocessing import LabelEncoder\n",
        "from cuml.svm import LinearSVC\n",
        "from cuml.preprocessing import Normalizer\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "cp._default_memory_pool.free_all_blocks()\n",
        "\n",
        "USE_CUDA = True\n",
        "\n",
        "if (USE_CUDA):\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zp8KLhKQUX_R"
      },
      "outputs": [],
      "source": [
        "# extracts faces from entire LFW dataset using MTCNN\n",
        "''' preprocessing_file = '../mtcnn_extracted_faces/'\n",
        "from_file = '../LFW_Dataset/lfw-deepfunneled/lfw-deepfunneled/' '''\n",
        "mtcnn = MTCNN(post_process=False, device=device)\n",
        "preprocessing_file = './mtcnn_extracted_faces/'\n",
        "# from_file = './lfw-deepfunneled/'\n",
        "from_file = '../LFW_Dataset/lfw-deepfunneled/lfw-deepfunneled/'\n",
        "\n",
        "\n",
        "list_directories = os.listdir(from_file)\n",
        "for dir in list_directories:\n",
        "  save_path = preprocessing_file + dir + '/'\n",
        "  if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "  curr_path = from_file + dir\n",
        "  curr_files = [curr_path + '/' + image for image in os.listdir(curr_path)]\n",
        "  for i, curr_img in enumerate(curr_files):\n",
        "    frame = Image.open(curr_img).convert(\"RGB\")\n",
        "    face = mtcnn(frame)\n",
        "\n",
        "    if face is None:\n",
        "      continue\n",
        "    img=Image.fromarray(np.uint8(face.permute(1,2,0).int().numpy()))\n",
        "    save_name = save_path + dir + str(i).zfill(4) + '.jpg'\n",
        "    img.save(save_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FIxrm16xbRTe"
      },
      "outputs": [],
      "source": [
        "#!rm -rf lfw-deepfunneled/\n",
        "#!tar -xvzf lfw-deepfunneled.tgz\n",
        "#!tar -cvzf mtcnn_extracted_faces.tar.gz mtcnn_extracted_faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "86Yh2nRdUX_S"
      },
      "outputs": [],
      "source": [
        "def load_dataset(directory, batch_size=1000, min_faces=0):\n",
        "  train_x, train_y, test_x, test_y = [],[], [],[]\n",
        "  batch_num = 0\n",
        "\n",
        "  # add slash to end of dir path\n",
        "  if directory[-1] != '/':\n",
        "    directory += '/'\n",
        "\n",
        "  for i, subdir in enumerate(listdir(directory)):\n",
        "\n",
        "    if ((i % batch_size) == 0) and i != 0:\n",
        "      savez_compressed('lfw-deepfunneled-dataset_{}.npz'.format(str(batch_num).zfill(4)),train_x,train_y,test_x,test_y)\n",
        "\n",
        "      train_x, train_y, test_x, test_y = [],[], [],[]\n",
        "      batch_num += 1\n",
        "\n",
        "    path = directory + subdir + '/'\n",
        "    #load all faces in subdirectory\n",
        "    faces = [asarray(Image.open(path + img_name).convert(\"RGB\")) for img_name in listdir(path)]\n",
        "    if min_faces > 0 and len(faces) < min_faces:\n",
        "      continue\n",
        "    if len(faces)>1:\n",
        "      test_x.append(faces.pop())\n",
        "      test_y.append(subdir)\n",
        "    labels = [subdir for _ in range(len(faces))]\n",
        "    # print(\"%d There are %d images in the class %s:\"%(i,len(faces),subdir))\n",
        "    train_x.extend(faces)\n",
        "    train_y.extend(labels)\n",
        "  # return asarray(train_x),asarray(train_y), asarray(test_x), asarray(test_y)\n",
        "\n",
        "  if not (train_x == [] and train_y == [] and test_x == [] and test_y == []):\n",
        "    savez_compressed('lfw-deepfunneled-dataset_{}.npz'.format(str(batch_num).zfill(4)),train_x,train_y,test_x,test_y)\n",
        "\n",
        "\n",
        "load_dataset('./mtcnn_extracted_faces/', 1000, 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amq1V-tNUX_S",
        "outputId": "ca7a2051-e81f-4572-d40f-43f93a776478"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 21:53:18.078884: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:18.079434: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:18.079568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:18.080802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:18.080926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:18.081025: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:20.539686: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:20.539844: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:20.539959: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-14 21:53:20.540054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3393 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lfw-deepfunneled-dataset_0002.npz', 'lfw-deepfunneled-dataset_0003.npz', 'lfw-deepfunneled-dataset_0000.npz', 'lfw-deepfunneled-dataset_0005.npz', 'lfw-deepfunneled-dataset_0004.npz', 'lfw-deepfunneled-dataset_0001.npz']\n",
            "(1737, 160, 160, 3) (1737,) (278, 160, 160, 3) (278,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-14 21:53:25.774793: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 7s 52ms/step\n",
            "9/9 [==============================] - 2s 227ms/step\n",
            "new_trainx (1737, 512) | new_testx (278, 512) | dtype float32\n",
            "new_trainy 1737 | new_testy 278 \n",
            "(1842, 160, 160, 3) (1842,) (298, 160, 160, 3) (298,)\n",
            "58/58 [==============================] - 3s 54ms/step\n",
            "10/10 [==============================] - 2s 169ms/step\n",
            "new_trainx (3579, 512) | new_testx (576, 512) | dtype float32\n",
            "new_trainy 3579 | new_testy 576 \n",
            "(1948, 160, 160, 3) (1948,) (298, 160, 160, 3) (298,)\n",
            "61/61 [==============================] - 3s 54ms/step\n",
            "10/10 [==============================] - 0s 27ms/step\n",
            "new_trainx (5527, 512) | new_testx (874, 512) | dtype float32\n",
            "new_trainy 5527 | new_testy 874 \n",
            "(1246, 160, 160, 3) (1246,) (233, 160, 160, 3) (233,)\n",
            "39/39 [==============================] - 3s 70ms/step\n",
            "8/8 [==============================] - 0s 27ms/step\n",
            "new_trainx (6773, 512) | new_testx (1107, 512) | dtype float32\n",
            "new_trainy 6773 | new_testy 1107 \n",
            "(2147, 160, 160, 3) (2147,) (269, 160, 160, 3) (269,)\n",
            "68/68 [==============================] - 3s 44ms/step\n",
            "9/9 [==============================] - 2s 207ms/step\n",
            "new_trainx (8920, 512) | new_testx (1376, 512) | dtype float32\n",
            "new_trainy 8920 | new_testy 1376 \n",
            "(2633, 160, 160, 3) (2633,) (303, 160, 160, 3) (303,)\n",
            "83/83 [==============================] - 2s 27ms/step\n",
            "10/10 [==============================] - 2s 191ms/step\n",
            "new_trainx (11553, 512) | new_testx (1679, 512) | dtype float32\n",
            "new_trainy 11553 | new_testy 1679 \n",
            "Final new_trainx size (11553, 512) | Final new_testx size (1679, 512)\n",
            "Final new_trainy size (11553,) | Final new_testy size (1679,)\n"
          ]
        }
      ],
      "source": [
        "#create and save embeddings\n",
        "embedder = FaceNet()\n",
        "#load the compressed dataset and facenet keras model\n",
        "dataset_npz_filenames = []\n",
        "reg_expr = '^lfw-deepfunneled-dataset_[0-9]{4}.npz'\n",
        "for file in listdir('./'):\n",
        "    if re.search(reg_expr, file):\n",
        "        dataset_npz_filenames.append(file)\n",
        "\n",
        "print(dataset_npz_filenames)\n",
        "new_trainy, new_testy  = [], []\n",
        "new_trainx = new_testx = np.zeros((0,512), dtype='float32')\n",
        "for i, file in enumerate(dataset_npz_filenames):\n",
        "    data = load(file)\n",
        "    trainx, trainy, testx, testy = data['arr_0'], data['arr_1'], data['arr_2'],  data['arr_3']\n",
        "    print(trainx.shape,trainy.shape,testx.shape, testy.shape)\n",
        "\n",
        "    new_trainx = np.vstack((new_trainx, embedder.embeddings(trainx)))\n",
        "    new_testx = np.vstack((new_testx, embedder.embeddings(testx)))\n",
        "    print('new_trainx {} | new_testx {} | dtype {}'.format(new_trainx.shape, new_testx.shape, new_testx.dtype))\n",
        "\n",
        "    for el in trainy:\n",
        "        new_trainy.append(el)\n",
        "    for el in testy:\n",
        "        new_testy.append(el)\n",
        "    print('new_trainy {} | new_testy {} '.format(len(new_trainy), len(new_testy)))\n",
        "\n",
        "new_trainy=np.array(new_trainy)\n",
        "new_testy=np.array(new_testy)\n",
        "\n",
        "#save the embeddings\n",
        "#compress the 512 embeddings of each face\n",
        "print(\"Final new_trainx size {} | Final new_testx size {}\".format(new_trainx.shape, new_testx.shape))\n",
        "print(\"Final new_trainy size {} | Final new_testy size {}\".format(new_trainy.shape, new_testy.shape))\n",
        "savez_compressed('lfw-deepfunneled-embeddings.npz',new_trainx,new_trainy,new_testx,new_testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h0TBqe1tUX_T"
      },
      "outputs": [],
      "source": [
        "# Load the compressed dataset and embeddings\n",
        "data = np.load('./lfw-deepfunneled-embeddings.npz')\n",
        "train_X, train_Y, test_X, test_Y = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "q7A2CN3lU9KZ",
        "outputId": "6829813c-41b0-46c5-a75e-bffee6571686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9965376853942871\n",
            "0.8403812050819397\n"
          ]
        }
      ],
      "source": [
        "#Linear SVC Results\n",
        "\n",
        "model=LinearSVC(C=10)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler_C()), ('pca', model)])\n",
        "\n",
        "#MinMaxScaling\n",
        "scaler=MinMaxScaler_C().fit(train_X)\n",
        "trainx =scaler.transform(train_X)\n",
        "testx = scaler.transform(test_X)\n",
        "\n",
        "#encode labels\n",
        "label_encoder = LabelEncoder().fit(train_Y)\n",
        "true_training_labels_encoded = label_encoder.transform(train_Y)\n",
        "\n",
        "model.fit(cp.asarray(trainx),cp.asarray(true_training_labels_encoded))\n",
        "\n",
        "\n",
        "#predict\n",
        "predict_train = model.predict(cp.asarray(trainx))\n",
        "predict_test = model.predict(cp.asarray(testx))\n",
        "\n",
        "#Accuracy\n",
        "true_test_labels_encoded = label_encoder.transform(test_Y)\n",
        "acc_train = accuracy_score(true_training_labels_encoded,predict_train)\n",
        "acc_test = accuracy_score(true_test_labels_encoded,predict_test)\n",
        "\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxNv7NeKC8qA",
        "outputId": "ded63af3-89de-4bbe-adec-798724757129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model at for training is 1.0\n",
            "Accuracy of model at for testing is 0.8874329958308517\n"
          ]
        }
      ],
      "source": [
        "#KNN-CPU\n",
        "\n",
        "#Scaling\n",
        "scaler=MinMaxScaler_C().fit(train_X)\n",
        "trainx =scaler.transform(train_X)\n",
        "testx = scaler.transform(test_X)\n",
        "\n",
        "#encode labels\n",
        "label_encoder = LabelEncoder_C().fit(train_Y)\n",
        "true_training_labels_encoded = label_encoder.transform(train_Y)\n",
        "true_test_labels_encoded = label_encoder.transform(test_Y)\n",
        "\n",
        "#draw graph\n",
        "N=trainx.shape[0]\n",
        "k = 1\n",
        "neigh = KNeighborsClassifier_C(n_neighbors = k).fit(trainx,true_training_labels_encoded)\n",
        "predict_train = neigh.predict(trainx)\n",
        "predict_test = neigh.predict(testx)\n",
        "print(\"Accuracy of model at for training is\",metrics_C.accuracy_score(true_training_labels_encoded, predict_train))\n",
        "print(\"Accuracy of model at for testing is\",metrics_C.accuracy_score(true_test_labels_encoded, predict_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# saving the models as pickle files\n",
        "import pickle\n",
        "with open('Philmon_SVC_Model', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "with open('Philmon_KNN_Model', 'wb') as file:\n",
        "    pickle.dump(neigh, file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SVM and KNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
